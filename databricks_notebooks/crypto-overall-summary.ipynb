{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67444d3b-2cbb-4dd4-be4e-283ab9b5d122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/connect/expressions.py:1017: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+----------+------------------+-----------------+-----------------+------------------+-------------+-------------+------------+-------------------+\n|s_no|coin    |total_days|overall_avg_price |overall_min_price|overall_max_price|stddev_avg_price  |opening_price|closing_price|price_change|percent_change     |\n+----+--------+----------+------------------+-----------------+-----------------+------------------+-------------+-------------+------------+-------------------+\n|1   |bitcoin |3         |9051455.26758016  |8989647.0        |9152960.0        |77450.61804051751 |9144583.0    |9000746.0    |-143837.0   |-1.5729202742213615|\n|2   |dogecoin|3         |16.306880684435907|15.66            |16.96            |0.6115487896069971|16.95        |15.7         |-1.25       |-7.374631268436578 |\n|3   |ethereum|3         |221096.25422813787|213520.0         |226039.0         |6373.263550331786 |226034.0     |213688.0     |-12346.0    |-5.462010140067424 |\n+----+--------+----------+------------------+-----------------+-----------------+------------------+-------------+-------------+------------+-------------------+\n\n✅ Saved overall summary with s_no to: wasbs://crypto-data@[REDACTED].blob.core.windows.net/output/overall_summary/2025-06-07_overall_summary.parquet\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, avg, min, max, stddev, count, first, last,\n",
    "    row_number\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "import datetime\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Secret-based config\n",
    "storage_account_name = dbutils.secrets.get(scope=\"cryptoSecret\", key=\"azure-storage-account-name\")\n",
    "storage_account_key = dbutils.secrets.get(scope=\"cryptoSecret\", key=\"azure-storage-account-key\")\n",
    "container_name = \"crypto-data\"\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)\n",
    "\n",
    "# Today's date\n",
    "today_str = datetime.datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define paths\n",
    "base_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net\"\n",
    "daily_summary_root = f\"{base_path}/output/daily_summary\"\n",
    "overall_out_path   = f\"{base_path}/output/overall_summary/{today_str}_overall_summary.parquet\"\n",
    "\n",
    "# ── 1. Discover valid daily summary folders ────────────────────────────────\n",
    "summary_dirs = []\n",
    "for item in dbutils.fs.ls(daily_summary_root):\n",
    "    try:\n",
    "        inner_files = dbutils.fs.ls(item.path)\n",
    "        if any(f.name.endswith(\".parquet\") for f in inner_files):\n",
    "            summary_dirs.append(item.path)\n",
    "    except Exception:\n",
    "        pass  # skip unreadable folders\n",
    "\n",
    "if not summary_dirs:\n",
    "    raise FileNotFoundError(\"No daily summary parquet directories found!\")\n",
    "\n",
    "# ── 2. Read and union all valid daily summary data ─────────────────────────\n",
    "df_all = spark.read.parquet(*summary_dirs)\n",
    "\n",
    "# ── 3. Cast date column to date type ───────────────────────────────────────\n",
    "df_all = df_all.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "\n",
    "# ── 4. Calculate first and last closing prices per coin across all dates ──\n",
    "w_date = Window.partitionBy(\"coin\").orderBy(\"date\")\n",
    "\n",
    "df_with_open_close = (\n",
    "    df_all\n",
    "    .withColumn(\"first_open\", first(\"opening_price\").over(w_date))\n",
    "    .withColumn(\"last_close\", last(\"closing_price\").over(w_date))\n",
    ")\n",
    "\n",
    "# ── 5. Aggregate overall stats per coin ────────────────────────────────────\n",
    "df_overall = (\n",
    "    df_with_open_close.groupBy(\"coin\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_days\"),\n",
    "        avg(\"avg_price\").alias(\"overall_avg_price\"),\n",
    "        min(\"min_price\").alias(\"overall_min_price\"),\n",
    "        max(\"max_price\").alias(\"overall_max_price\"),\n",
    "        stddev(\"avg_price\").alias(\"stddev_avg_price\"),\n",
    "        first(\"first_open\").alias(\"opening_price\"),\n",
    "        last(\"last_close\").alias(\"closing_price\")\n",
    "    )\n",
    "    .withColumn(\"price_change\", col(\"closing_price\") - col(\"opening_price\"))\n",
    "    .withColumn(\"percent_change\", (col(\"price_change\") / col(\"opening_price\")) * 100)\n",
    ")\n",
    "\n",
    "# ── 6. Add serial number (s_no) ordered by coin ────────────────────────────\n",
    "w_serial = Window.orderBy(\"coin\")\n",
    "df_overall_final = df_overall.withColumn(\"s_no\", row_number().over(w_serial))\n",
    "\n",
    "# Reorder columns: s_no first\n",
    "ordered_cols = [\"s_no\"] + [col_name for col_name in df_overall_final.columns if col_name != \"s_no\"]\n",
    "df_overall_final = df_overall_final.select(ordered_cols)\n",
    "\n",
    "# ── 7. Show and save ───────────────────────────────────────────────────────\n",
    "# df_overall_final.show(truncate=False)\n",
    "df_overall_final.write.mode(\"overwrite\").parquet(overall_out_path)\n",
    "print(f\"✅ Saved overall summary with s_no to: {overall_out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "158e20ca-d75d-4474-bad4-6959dfeebeca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "crypto-overall-summary",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}